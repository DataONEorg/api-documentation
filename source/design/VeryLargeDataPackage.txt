Very Large Data Packages
========================

:Document Status:
  ======== ==================================================================
  Status   Comment
  ======== ==================================================================
  DRAFT    (rnahf) Initial draft
  ======== ==================================================================

.. contents::

Synopsis
---------

While many data packages are of modest size (<100 objects), some large studies 
generate upwards of 100,000 datasets that form a data package.   These very large 
(VL) packages challenge performance limits in the DataONE data ingest cycle and
can present usability issues in user interfaces not prepared for them.  Both memory
and processor time increase dramatically with increased number of data objects
and relationships expressed.

Submitters of packages containing large numbers of data objects should be mindful
that packages of such an enormous number of objects is likely to be unusable for 
the majority of interested parties, and should consider consolidating and compressing
the individual objects into fewer objects to allow easier download and inspection.
This should be especially considered if the objects in the package would not be
usefully retrieved individually.

Creation of large resource maps is potentially the most time consuming activity,
depending on the tool used.  Deserialization is comparatively quick, but the memory
requirements are high, depending on the type of model used during parsing.  At the
stage of indexing, at issue is the time needed to process index record updates, as
well as the resulting number items in the solr records.  Finally, high-level client
methods would like to safely be able to do whole-package downloads, but need to be 
able to detect large data packages which could overwhelm their ability to handle
such as large package.

Below are the results of performance tests done to better understand the dynamics
involved.

Resource map creation
---------------------
Use of the foresite library for building resource maps includes many checks to make
sure that the map validates.  First the identifiers of the data and metadata are 
added to a graph held in memory, then the graph is serialized to RDF/XML format.
For small packages the overhead for building the graph and performing consistency 
checks is minimal, but both memory and time to build seem to scale geometrically 
with the number of objects in the package.  

Test results on different size resource maps are summarized below.  In all cases
there is one metadata object that documents all of the objects.

 # of objects    time to build             memory           Resulting file size
==============  ========================  ================  ===================
    10                                                               7 K
    33                                                              20 K
   100              2  seconds                45 MB                 60 K
   330                                                             192 K
  1000              6  seconds                20 MB                600 K
  3300             24  seconds                23 MB	   21/25         2 Mb      
 10000            4.5  minutes                30 MB     55           6 Mb            
 33000             66  minutes               142 MB           	    20 Mb
==============  ========================  ================  ==================


For creating very large resource maps, generation time using the java foresite 
toolkit is an issue.  Directly creating a serialized resource map is much faster.
For example, using an existing resource map as a template, and a short perl script, 
a 100000 member resource map was created in approximately 10 seconds with the only 
memory cost that of holding an identifier array in memory and any output buffering.



RDF Deserialization
---------------------
Deserialization happens both on the client side when downloading resource maps, 
and on coordinating nodes, both when validating the resource map, and also when
indexing the relationships into the solr index.  Performance metrics obtained 
from JUnit tests monitored with Java Visual VM are summarized below.  Fully
expressed resource maps were deserialized using both the default simple model,
and again using an OWL model loaded with the ORE schema to be able to do semantic 
reasoning.  The reasoning model adds an additional 268 triples from the ORE schema. 


==========  =======  ========  ========  =======  ========  ========
               Default (simple) model     Reasoning (OWL) model
            ---------------------------  ---------------------------
 # objects  triples   time      memory   triples   time      memory 
==========  =======  ========  ========  =======  ========  ========
    10           61    1 sec.     9 Mb       329    2 sec.    13 Mb 
    33          176    1 sec.    10 Mb       444    2 sec.    13 Mb            
   100          511    2 sec.    15 Mb       779    2 sec.    17 Mb            
   330         1661    2 sec.    20 Mb      1929    3 sec.    17 Mb 
  1000         5011    2 sec.    17 Mb      5279    3 sec.    24 Mb 
  3300        16511    3 sec.    20 Mb     16779    4 sec.    40 Mb 
 10000        50011    6 sec.    30 Mb     50279    8 sec.    90 Mb            
 33000       165011    7 sec.    51 Mb    165279   10 sec.   264 Mb            
100000       500011   15 sec.   138 Mb    500279   26 sec.   792 Mb 
==========  =======  ========  ========  =======  ========  ========

The same information listed by model size shows that for small models, one can 
see that memory requirements are not a simple function of number of triples, but
also a function of the model type.  The reasoning model uses more memory per
triple than the simple model.  Especially noticeable is that at very large sizes,
in terms of number of triples, the reasoning model uses significantly more memory.
The somewhat irregular increases indicate that perhaps this is due to pre-allocation
of memory in larger blocks as the number of items increases.

triples    time     memory   model type
========  =======  ========  ==========
     61	   1 sec.    9 Mb      simple
    176    1 sec.   10 Mb      simple
    329    2 sec.   13 Mb     reasoning
    444    2 sec.   13 Mb     reasoning
    511    2 sec.   15 Mb      simple
    779    2 sec.   17 Mb     reasoning
   1661    2 sec.   20 Mb      simple
   1929    3 sec.   17 Mb     reasoning
   5011    2 sec.   17 Mb      simple
   5279    3 sec.   24 Mb     reasoning
  16511    3 sec.   20 Mb      simple
  16779    4 sec.   40 Mb     reasoning
  50011    6 sec.   30 Mb      simple
  50279    8 sec.   90 Mb     reasoning
 165011    7 sec.   51 Mb      simple
 165279   10 sec.  264 Mb     reasoning
 500011   15 sec.  138 Mb      simple
 500279   26 sec.  792 Mb     reasoning
 ========  =======  ========  ==========


Indexing
---------
When resource maps are synchronized, the map is read and - once all of the package
members are indexed - the relationships in the map are added to the index records
of the data members.  A 10000 member package will trigger the update of 10000
index records, adding the metadata object pid to the 'isDocumentedBy' field.  
Additionally, both the 'contains' field in the resource map and the 'documents' 
field in the metadata records will be updated with the pids of the 10000 members.
Such many-membered fields are difficult to impossible to display, and are time-
consuming to search when queried.

Indexing is by necessity a single-threaded process, one that can update on the 
order of 100 records/minute.  Therefore


Whole-Package Download
------------------------
The high-level DataPackage.download(packageID) method in d1_libclient implementations
by default downloads the entire collection of data package objects for local usage.  
For VL packages, this could be terabytes of information.  In order to better support
such convenience features, there needs to be ways for determining the number of
members of a package prior to download.  

Determining Member Count
------------------------
It is useful for applications to know when a given data package is too large for 
it to work with.  Ideally, this could be determined before deserializing the xml,
and even for some clients, prior to download of the resource map itself.  

For indexed resource maps, the easiest way to get the member count is with the 
query::

  cn/v1/query/solr/?q=resourceMap:{pid}&rows=0
  
This does not estimate the size of the model or the memory requirements.

TODO: define followup query for total package size q=resourceMap:{pid}&fl=size&rows={rowcount} 


Determining Memory Requirements for deserialization
---------------------------------------------------
It is the number of triples and type of model used, moreso than the number of 
package members, that best determines the graph model's memory requirement, and 
so any additional triples expressed for each member would multiply the model size. 
The use of ORE proxies, for example, or the inclusion of provenance information 
are situations where this would be the case.  DataONE is planning for the 
inclusion of provenance statements in the resource maps, so users and developers 
alike should take this into consideration.

The number of triples in an RDF/XML file can be determined either by parsing the
XML, or by estimating off the resource map byte count.  By parsing the XML, one
would use an XML parser of choice to count all of the sub-elements of all of the 
"rdf:Description" elements.  In psuedo-code::

  tripleCount = 0;
  descriptionList = getRDFDescriptionElements();
  foreach descriptionElement in descriptionList {
     tripleCount += descriptionElement.getElements().size;
  }

The number of triples can then be used to 
To estimate from the file size, an upper limit of the number of triples can deduced.
RDF/XML organizes triples as predicate-object sub-elements under an rdf:Description
element for each subject. If the ratio of subjects to triples is low, then the number
of bytes per triple is determined by the length of the predicate-object sub-element.
For a 30-character identifier, that sub-element is about 100 characters, and so::


  upper limit on the number of triples = file size (bytes) / 100 bytes-per-triple
  
So for example, a 5Mb resource map has at most 50K triples, assuming an average
identifier size of 30 characters (URL encoded).  

For a point of reference, a resource map for 1 metadata object documenting 1000 
objects, expressing the 'ore:aggregates', 'ore:isAggregatedBy', 'cito:documents', 
'cito:isDocumentedBy', and 'cito:identifier' predicates creates 5005 triples using 
1003 subjects, and was tested to create 600K file.  Applying the upper limit 
approximation, (600K / 100 = 6K) gives 6000 triples, an over-estimate matching 
the number of subjects.

Note that long identifiers and identifiers predominated by non-ascii characters
that would be percent encoded in the file (3bytes per character) can lead to 
an even higher upper limit than expected, and similarly, short identifiers in 
the resource map could lead to a less robust upper limit.
