Log Aggregation Overview
========================

:Revisions: 
  ======== ============================================================ 
  Date     Comment 
  ======== ============================================================ 
  20111201 (Waltz) First draft
  ======== ============================================================ 

Introduction
------------

Logs will need to be aggregated from all Membernodes and stored at each
Coordinating Node.  Each Coordinating Node must replicas its logging data to
the other Coordinating Nodes.  The process described below relies on several
technologies: Quartz Scheduler, Hazelcast Data Distribution, Metacat 
Repository Storage, and the Solr Search platform.

DataONE use cases
..................................

.. toctree::
   :maxdepth: 1
   UseCases/16_uc
   UseCases/17_uc
   
..................................   


Log Aggregation Scheduling
--------------------------

Log Aggregation will performed once a day per membernode. There is no set time 
as to when harvesting of log records will be performed on a daily basis. 
At the time it is performed, it will harvest all the records from the last 
run until the beginning of the current day (00:00:00). Therefore, the aggregate 
logs on the CN may be only as recent as 48 hrs in the past, depending on when a 
query is executed and when the last harvesting was run.  In the LogScheduler 
class, Quartz will be used as the mechanism to schedule log harvesting.

Log Aggregation Distibuted Execution
------------------------------------

The Log Aggregator will run as a distributed executable class for each 
Member node, as well as locally for the metacat instance that acts as the
storage mechanism for the Coordinating Node.  The distribution of the execution
will be handled by Hazelcast.  There is a maximum number of executions of
the Log Aggretator that may be running at any given time (# of CNs * max per CN).

The Log Aggretator task will run a getLogRecords query against a Membernode. It
will loop through the results, calling LogPublisher for each LogEntry, and place 
on a distributed hazelcast topic a LogEntry to be broadcast to all members 
of the Hazelcast cluster (including itself).

Log Aggregation Indexing
------------------------

Each CoordinatingNode will have a Log Aggregator will be running a LogSubscriber
that listens for events on the distributed hazelcast topic.  For each LogEntry 
message received, it will call LogIndexer.  LogIndexer will take the LogEntry
and create a Solr Document finally submitting the document to Solr for addition
to the lucene index.  


.. figure:: images/logAggregator_seq.png

*Figure 1.* Sequence diagram illustrating the sequence for Log Aggregator process.

.. 
	@startuml images/logAggregator_seq.png
	
		participant "Log Scheduler" as CnSchd << CN >>
		participant "Core API" as CnCore << CN >>
		participant "Log Aggregator" as CnAgg << CN >>
		participant "Core API" as MnCore << MN >>
		participant "Log Publisher" as CnPub << CN >>
		participant "Log Subscriber" as CnSub << CN >>
		participant "Log Indexer" as CnInd << CN >>
		
		
		
		CnSchd -> CnSchd : init()
		activate CnSchd
		CnSchd -> CnCore : CNCore.listNodes()
		activate CnCore
		CnCore -> CnSchd: NodeList
		deactivate CnCore
		CnSchd -> CnSchd : Schedule all MNs in quartz
		CnSchd -> CnAgg : Trigger Aggregation of MN
		activate CnAgg
		
		deactivate CnSchd
		
		CnAgg->MnCore : getLogRecords(session, fromDate, toDate)
		activate MnCore
		MnCore->CnAgg : Log
		
		deactivate MnCore
		
		CnAgg->CnPub : scheduleEvent(LogEvent)
		activate CnPub
		CnPub->CnPub : topic.publish(LogEvent)
		
		deactivate CnPub
		
		deactivate CnAgg
		
		CnSub -> CnSub : onMessage(LogEvent)
		activate CnSub
		CnSub -> CnInd : indexItem(LogEvent)
		activate CnInd
		CnInd -> CnInd : create LogEvent in Solr
		
		deactivate CnSub
		
		deactivate CnInd
	@enduml

