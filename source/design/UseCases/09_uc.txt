.. _UC09:

Use Case 09 - Replicate MN to MN
--------------------------------

.. index:: Use Case 09, UC09, Replicate MN, replicate

Revisions
  View document revision history_.

Goal
  Replicate data from Member Node to Member Node.

Summary 
  Replication of content between Member Nodes (MN) is done to improve
  persistence of information (avoid data loss with loss of MN) and to improve
  accessibility (more choices for content retrieval can lower bandwidth
  requirements for any particular MN). The process of replication is controlled
  by a Coordinating Node (CN). 

  A full copy of science data and metadata is made during the replication
  process, so the original science metadata and data is copied to the
  recipient MN.

  Data is copied across as an exact copy. Science metadata may be transformed
  into another format if the original can not be supported. 

  It is important that the original metadata is preserved on the CNs, as
  it is always possible that the original MN where the content was published
  may go offline or be removed from the DataONE system.


Actors
  Two Member Nodes, one or more Coordinating Nodes

Preconditions 
  - Content is present on a Member Node

  - The content has been registered with the DataONE system (i.e. Member Node
    Synchronization has occurred for the data and metadata)

Triggers
 - A Coordinating Node detects that there are insufficient copies of the
   object(s) in question.

 - Information on a Member Node is altered

 - Capabilities of a Member Node changes (accepting more or less content)

 - Replication policy of DataONE or a Member Node changes

 - A Member Node goes offline


Post Conditions
  - Content is present on the recipient Member Node

  - System metadata is updated to reflect the change

  - Watchers are notified of the change

  - Member Node and Coordinating Node logs are updated


.. 
   @startuml images/09_uc.png
   usecase "12. Authentication" as authen
   package "DataONE"
     actor "Coordinating Node" as CN
     actor "Member Node 1" as MN1
     actor "Member Node 2" as MN2
     usecase "13. Authorization" as author
     usecase "01. Get Object" as GET
     usecase "04. Create object" as CREATE
     usecase "06. Synchronize content" as SYNC
     usecase "16. Log event" as log
     usecase "21. Notify subscribers" as subscribe
     CN -- CREATE
     CN -- SYNC
     MN1 -- CREATE
     MN2 -- GET
     MN1 -- GET
     GET ..> author: <<includes>>
     GET ..> authen: <<includes>>
     GET ..> log: <<includes>>
     GET ..> subscribe: <<includes>>
     CREATE ..> author: <<includes>>
     CREATE ..> log: <<includes>>
     CREATE ..> subscribe: <<includes>>
    @enduml

.. image:: images/09_uc.png

*Figure 1.* Use case diagram indicating the actors involved in the process of
Member Node replication.

..
  @startuml images/09_uc.png
  skinparam notebordercolor #AAAAAA
  skinparam notefontcolor #222222
  title Replicate an object between two Member Nodes\n\n
  participant "mnA : MNode" as mnA <<MNode>>
  participant "mnB : MNode" as mnB <<MNode>>
  participant "cnXReplService : ReplicationService" as cnXrepl <<CNode>>
  participant "cnZReplService : ReplicationService" as cnZ <<CNode>>
  
  == Replication Event  ==
  [-> cnXrepl : hzSystemMetadata.put(pid, sysmeta)
  activate cnXrepl #D74F57
  
  note right
    Synchronization services adds entry to 
    SystemMetadata map managed by Hazelcast,
    EntryEvent is fired
  end note
  
  cnXrepl -> cnXrepl : entryAdded(EntryEvent<pid, sysmeta>)
  cnXrepl -> cnXrepl : createReplicationTaskList(pid)
  
  loop for each ReplicationTask
  cnXrepl -> cnXrepl : taskid = idGenerator.newId()
    note right
    Hazelcast.getIdGenerator("task-ids") has been
    called in ReplicationService constructor 
  end note
  cnXrepl -> cnXrepl : hzReplicationTaskQueue.put(taskid, task)
  end loop
  note right
    Hazelcast distributes Replication
    list to other CNs
  end note
  
  deactivate cnXrepl

     
  == Process Replication Tasks ==
    
  note right
  Each ReplicationService
  continuously polls the 
  hz queues
  end note
  
          
  cnZ -> cnZ: entryAdded(EntryEvent<taskid, task>)
  activate cnZ #D74F57
  cnZ -> cnZ: hzReplicationTaskQueue.poll()
  cnZ --> cnZ: task

  cnZ -> cnZ: ExecutorService.submit(task)
  activate cnZ #DarkSalmon
  cnZ -> cnZ: replicationTask.call(pid)
  cnZ -> mnB: replicate(cnZSession, mnASession, pid)
  activate mnB #D74F57
  mnB --> cnZ: replicateResponse
  deactivate cnZ 
  cnZ -> cnZ: setReplicationStatus(pid, ReplicationStatus.QUEUED)
  
  note left
   Object's system metadata get's updated
  end note
  
  cnZ -> cnZ: hzSystemMetadata.lock(pid)
  cnZ -> cnZ: updateSystemMetadata(pid)
  cnZ -> cnZ: hzSystemMetadata.unlock(pid)
  cnZ --> cnZ: statusResponse
  
  mnB -> mnA: getReplica(mnBSession, pid)
  deactivate mnB
  activate mnA #D74F57
  mnA -> cnZ: isNodeAuthorized(mnASession, mnBSubject, pid, replicationPermission)
  cnZ --> mnA: authorizationResponse
  mnA --> mnB: replicaBytes
  deactivate mnA
  activate mnB #D74F57
  
  mnB -> cnZ: setReplicationStatus(pid, ReplicationStatus.COMPLETE)
  deactivate mnB
  cnZ -> mnA: mnA.getChecksum(pid)
  activate mnA #D74F57
  mnA --> cnZ : checksum
  
  note right
  Object's system metadata get's updated
  end note
  
  deactivate mnA
  cnZ -> cnZ: hzSystemMetadata.lock(pid)
  cnZ -> cnZ: updateSystemMetadata(pid)
  cnZ -> cnZ: hzSystemMetadata.unlock(pid)
  cnZ --> cnZ: statusResponse
  deactivate cnZ
  @enduml


.. image:: images/09_seq.png

*Figure 2.* Interactions for use case 09. The diagram describes transfer of a
single object from MN_A to MN_B as directed by a CN. It is assumed that the
object does not exist on MN_A and the object has been identified as requiring
replication by the CN checking its status in the system metadata. The end 
state of a replicate operation is that content is available on the MN, the 
MN has notified the CN of such, and the CN will schedule a synchronize 
operation that will verify the copy as legitimate.


Note (2011.01.07 CWB): This simple authentication scheme will not work on
member nodes that have their own access control rules. In this scheme, each
member node will need to have knowledge of the administrative (or replication)
credentials for each of the other member nodes. The CN needs to handle the
login actions for both of the MNs involved and send an authenticated token
from MN_A to MN_B so that it can use that credential to successfully get the
document. This is only the case if the document on MN_A is read protected. If
it is public, not token is needed.

Note that the call setReplicationStatus with a value of *COMPLETE* is 
functionally equivalent to the *notify(objectCreated, identifier)* call 
indicated in use case 06.

.. _history: https://redmine.dataone.org/projects/d1/repository/changes/documents/Projects/cicore/architecture/api-documentation/source/design/UseCases/09_uc.txt
